{"cells":[{"cell_type":"code","source":["\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import input_file_name,col, lit , current_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType,LongType\n","\n","\n","csv_schema = StructType([\n","    StructField(\"source_id\", LongType(), True),\n","    StructField(\"ra\", DoubleType(), True),\n","    StructField(\"dec\", DoubleType(), True),\n","    StructField(\"parallax\", DoubleType(), True),\n","    StructField(\"parallax_error\", DoubleType(), True),\n","     StructField(\"parallax_over_error\", DoubleType(), True),\n","    StructField(\"pm\", DoubleType(), True),\n","     StructField(\"pmra\", DoubleType(), True),\n","    StructField(\"pmdec\", DoubleType(), True),\n","     StructField(\"phot_g_mean_mag\", DoubleType(), True),\n","     StructField(\"teff_gspphot\", DoubleType(), True),\n","    StructField(\"distance_gspphot\", DoubleType(), True),\n","    #StructField(\"processing_timestamp\", TimestampType(), True)\n","])\n","\n","# 2. Define the path to the directory Spark should monitor for new CSV files\n","input_csv_directory = \"Files/Staging\" \n","\n","# Ensure the directory exists (though Spark will monitor it even if initially empty)\n","print(f\"Monitoring directory for new CSVs: {input_csv_directory}\")\n","\n","# 3. Read from the directory as a stream\n","# Spark will automatically discover new files added to this directory.\n","# It processes files based on their modification timestamps or names.\n","streaming_df = spark.readStream \\\n","    .format(\"csv\") \\\n","    .schema(csv_schema) \\\n","    .option(\"header\", \"true\") \\\n","    .load(input_csv_directory)\n","PARSEC_TO_METERS = 3.08567758*1e16\n","# You can add transformations here if needed, for example:\n","transformed_streaming_df = streaming_df \\\n","   .withColumn(\"distance\", col(\"distance_gspphot\")*PARSEC_TO_METERS) \\\n","   .withColumn(\"processing_timestamp\",current_timestamp()) \\\n","   .withColumn(\"source_file\", input_file_name())\n","\n","\n","# 4. Write the stream to a Delta table\n","delta_table_name = \"stg_gaia\" # Your target Delta table name\n","checkpoint_location = f\"Files/checkpoints/{delta_table_name}_checkpoint\" # MUST be a unique, reliable path in OneLake\n","\n","# Ensure the checkpoint directory exists or Spark has permissions to create it\n","\n","# This is the simplest streaming sink.\n","query_append = (transformed_streaming_df.writeStream \\\n","    .format(\"delta\") \\\n","    .outputMode(\"append\") \\\n","    .option(\"checkpointLocation\", checkpoint_location) \\\n","    .trigger(availableNow=True) \\\n","    .start(f\"abfss://b3979a08-4bbe-4d35-b864-4da7d2c8b2b4@onelake.dfs.fabric.microsoft.com/608b9b1d-a537-4410-abb1-240bd8dc87ff/Tables/dbo/stg_gaia\"))\n","\n","# Wait for the streaming query to finish\n","query_append.awaitTermination() # Writes to a metastore-registered Delta table\n","   \n","\n","print(f\"Streaming query started in append mode. New CSVs dropped in '{input_csv_directory}' will be loaded into '{delta_table_name}'.\")\n","print(f\"Checkpoint location: {checkpoint_location}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"cancelled","session_id":"1b82218c-0272-43a0-b906-f75c6732565c","normalized_state":"cancelled","queued_time":"2025-05-31T17:09:18.3021861Z","session_start_time":null,"execution_start_time":"2025-05-31T17:09:18.3035056Z","execution_finish_time":"2025-05-31T17:11:33.6932951Z","parent_msg_id":"77c21fd8-ea78-4feb-8816-35d28c60f224"},"text/plain":"StatementMeta(, 1b82218c-0272-43a0-b906-f75c6732565c, 9, Finished, Cancelled, Cancelled)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Monitoring directory for new CSVs: Files/Staging\n"]},{"output_type":"stream","name":"stdout","text":["Streaming query started in append mode. New CSVs dropped in 'Files/Staging' will be loaded into 'stg_gaia'.\nCheckpoint location: Files/checkpoints/stg_gaia_checkpoint_test5\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52d1420e-62f2-438e-8832-2f17a52f1f6e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"608b9b1d-a537-4410-abb1-240bd8dc87ff","known_lakehouses":[{"id":"608b9b1d-a537-4410-abb1-240bd8dc87ff"}],"default_lakehouse_name":"Gaia_Lake","default_lakehouse_workspace_id":"b3979a08-4bbe-4d35-b864-4da7d2c8b2b4"}}},"nbformat":4,"nbformat_minor":5}